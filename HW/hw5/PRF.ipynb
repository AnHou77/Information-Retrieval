{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pylab import random\n",
    "import math\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "from collections import Counter\n",
    "from tqdm import tqdm \n",
    "from numba import jit,njit\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "30000 30000 30000\n150 150\n29735 29735 29735\n154240 324\n"
     ]
    }
   ],
   "source": [
    "# 使用 nltk PlaintextCorpusReader 存取指定目錄下的所有檔案\n",
    "doc_corpus_root = './ntust-ir-2020/docs'\n",
    "query_corpus_root = './ntust-ir-2020/queries'\n",
    "doc_corpus = PlaintextCorpusReader(doc_corpus_root, '.*')\n",
    "query_corpus = PlaintextCorpusReader(query_corpus_root, '.*')\n",
    "\n",
    "# 將 document 依序 存取內容與檔名\n",
    "all_words = {}\n",
    "word2id = {}\n",
    "id2word = {}\n",
    "doc_names = []\n",
    "doc_terms = []\n",
    "for docs in doc_corpus.fileids():\n",
    "    words = doc_corpus.words(docs)\n",
    "    word_dict = dict(Counter(words))\n",
    "    for w in range(len(word_dict)):\n",
    "        k = list(word_dict.keys())[w]\n",
    "        v = list(word_dict.values())[w]\n",
    "        if k in all_words:\n",
    "            all_words[k] += v\n",
    "        else:\n",
    "            all_words[k] = v\n",
    "    # 檔名篩掉.txt\n",
    "    doc_names.append(docs[:len(docs) - 4])\n",
    "    doc_terms.append(word_dict)\n",
    "\n",
    "query_words = {}\n",
    "# 將 query 依序 存取內容與檔名\n",
    "query_names = []\n",
    "query_terms = []\n",
    "for queries in query_corpus.fileids():\n",
    "    words = query_corpus.words(queries)\n",
    "    word_dict = dict(Counter(words))\n",
    "    for w in range(len(word_dict)):\n",
    "        k = list(word_dict.keys())[w]\n",
    "        v = list(word_dict.values())[w]\n",
    "        if k in query_words:\n",
    "            query_words[k] += v\n",
    "        else:\n",
    "            query_words[k] = v\n",
    "    # 檔名篩掉.txt\n",
    "    query_names.append(queries[:len(queries) - 4])\n",
    "    query_terms.append(word_dict)\n",
    "\n",
    "filtered_word = {}\n",
    "i = 0\n",
    "for w in all_words.keys():\n",
    "    if w in query_words or all_words[w] > 10:\n",
    "        filtered_word[w] = all_words[w]\n",
    "        word2id[w] = i\n",
    "        id2word[i] = w\n",
    "        i += 1\n",
    "\n",
    "filtered_doc_terms = []\n",
    "for doc in doc_terms:\n",
    "    word_dict = {}\n",
    "    for i in range(len(doc.keys())):\n",
    "        word = list(doc.keys())[i]\n",
    "        if word in filtered_word:\n",
    "            word_dict[word] = doc[word]\n",
    "    filtered_doc_terms.append(word_dict)\n",
    "\n",
    "\n",
    "# 確認長度相符\n",
    "print(len(doc_names),len(doc_terms),len(filtered_doc_terms))\n",
    "print(len(query_names),len(query_terms))\n",
    "print(len(word2id),len(id2word),len(filtered_word))\n",
    "print(len(all_words),len(query_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tfidf(queries,docs,words):\n",
    "    \n",
    "    word_lens = len(words)\n",
    "    doc_lens = len(queries) + len(docs)\n",
    "    tf_words = np.zeros((doc_lens,word_lens))\n",
    "    idf_words = np.zeros(word_lens)\n",
    "    n = 0\n",
    "    \n",
    "    for j in tqdm(range(doc_lens)):\n",
    "        if j < 150:\n",
    "            terms = queries\n",
    "            n = j\n",
    "        else:\n",
    "            terms = docs\n",
    "            n = j - 150\n",
    "        for i in range(word_lens):\n",
    "            if id2word[i] in terms[n]:\n",
    "                # tf sublinear\n",
    "                tf_words[j][i] = np.log(terms[n][id2word[i]]) + 1\n",
    "                idf_words[i] += 1\n",
    "            if j == (doc_lens - 1):\n",
    "                # idf smoothing\n",
    "                idf_words[i] = math.log(1 + (float(doc_lens) - idf_words[i] + 0.5) / (idf_words[i] + 0.5))\n",
    "    \n",
    "    tfidf = np.zeros((doc_lens,word_lens))\n",
    "    \n",
    "    for j in range(doc_lens):\n",
    "        if j < 150:\n",
    "            terms = queries\n",
    "            n = j\n",
    "        else:\n",
    "            terms = docs\n",
    "            n = j - 150\n",
    "        words = list(terms[n].keys())\n",
    "        for w in words:\n",
    "            i = word2id[w]\n",
    "            tfidf[j][i] = tf_words[j][i] * idf_words[i]\n",
    "\n",
    "    return tf_words, idf_words, tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def get_bm25_matrix(doc_tf,doc_idf,query_tf,avg_doc_words,doc_words,k1 = 1, k3 = 1000, b = 0.85):\n",
    "    \n",
    "    doc_len = len(doc_tf)\n",
    "    query_len = len(query_tf)\n",
    "    \n",
    "    tfidf = np.zeros((query_len,doc_len))\n",
    "    \n",
    "    for i in range(query_len):\n",
    "        for j in range(doc_len):\n",
    "            for q in range(len(query_tf[i])):\n",
    "                q_tf = query_tf[i][q]\n",
    "                if q_tf != 0:\n",
    "                    _f = doc_tf[j][q] / (1 - b + b * doc_words[j] / avg_doc_words)\n",
    "                    w_d = (k1 + 1) * (_f + 0.5) / (k1 + _f + 0.5)\n",
    "                    w_q = (k3 + 1) * q_tf / (k3 + q_tf)\n",
    "                    tfidf[i][j] += doc_idf[q] * w_d * w_q\n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rocchio(query_tfidf,doc_tfidf,bm25,a=1,b=0.75,r=0,step=7,rel=5,nrel=1):\n",
    "    \n",
    "    # get vsm matrix\n",
    "    vsm = cosine_similarity(query_tfidf,doc_tfidf)\n",
    "    # combine vsm with bm25\n",
    "    score = vsm * bm25\n",
    "    # sort score matrix\n",
    "    rank = np.flip(score.argsort(), axis=1)\n",
    "    \n",
    "    for _ in tqdm(range(step)):\n",
    "        # record rel_docs\n",
    "        rels = doc_tfidf[rank[:, :rel]].mean(axis=1)\n",
    "        # record nonrel_docs\n",
    "        nrels = doc_tfidf[rank[:, -nrel:]].mean(axis=1)\n",
    "        # rewrite query tfidf\n",
    "        query_tfidf = a * query_tfidf + b * rels - r * nrels\n",
    "\n",
    "        # use new query tfidf get vsm matrix\n",
    "        vsm = cosine_similarity(query_tfidf,doc_tfidf)\n",
    "        score = vsm * bm25\n",
    "        rank = np.flip(score.argsort(axis=1), axis=1)\n",
    "\n",
    "    return rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 30150/30150 [03:32<00:00, 142.05it/s]\n",
      "(30150, 29735) (29735,)\n"
     ]
    }
   ],
   "source": [
    "# get tf,idf,tfidf\n",
    "tf,idf,tfidf = get_tfidf(query_terms,filtered_doc_terms,filtered_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 30000/30000 [00:01<00:00, 20325.95it/s]\n"
     ]
    }
   ],
   "source": [
    "# seperate query & doc tfidf\n",
    "query_tfidf = tfidf[:150,:]\n",
    "doc_tfidf = tfidf[150:,:]\n",
    "# seperate query & doc tf\n",
    "query_tf = tf[:150,:]\n",
    "doc_tf = tf[150:,:]\n",
    "\n",
    "avg_doc_words = sum(all_words.values()) / len(doc_tf)\n",
    "doc_words = np.zeros(len(doc_tf))\n",
    "for j in tqdm(range(len(doc_tf))):\n",
    "    doc_words[j] = sum(doc_terms[j].values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get bm25 matrix\n",
    "bm25_tfidf = get_bm25_matrix(doc_tf,idf,query_tf,avg_doc_words,doc_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 7/7 [04:32<00:00, 38.92s/it]\n"
     ]
    }
   ],
   "source": [
    "# get rocchio ranking\n",
    "ranking = rocchio(query_tfidf,doc_tfidf,bm25_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 讀檔、寫入答案\n",
    "ans = \"Query,RetrievedDocuments\"\n",
    "f = open(\"rocchio_result.txt\",\"w+\")\n",
    "f.write(ans+'\\n')\n",
    "\n",
    "buf = \"\"\n",
    "for i in range(len(query_names)):\n",
    "    buf = query_names[i] + ','\n",
    "    first = True\n",
    "\n",
    "    for s in range(5000):\n",
    "        if first == True:\n",
    "            buf += doc_names[ranking[i][s]]\n",
    "        else:\n",
    "            buf += (' ' + doc_names[ranking[i][s]])\n",
    "        first = False\n",
    "\n",
    "    f.write(buf+'\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.3 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}