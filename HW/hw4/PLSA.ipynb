{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pylab import random\n",
    "import math\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "from collections import Counter\n",
    "from tqdm import tqdm \n",
    "from numba import jit,njit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "14955 14955 14955\n100 100\n226 226 226\n111449 226\n(14955, 226)\n"
     ]
    }
   ],
   "source": [
    "# 使用 nltk PlaintextCorpusReader 存取指定目錄下的所有檔案\n",
    "doc_corpus_root = './ntust-ir-2020/docs'\n",
    "query_corpus_root = './ntust-ir-2020/queries'\n",
    "doc_corpus = PlaintextCorpusReader(doc_corpus_root, '.*')\n",
    "query_corpus = PlaintextCorpusReader(query_corpus_root, '.*')\n",
    "\n",
    "# 將 document 依序 存取內容與檔名\n",
    "all_words = {}\n",
    "word2id = {}\n",
    "id2word = {}\n",
    "doc_names = []\n",
    "doc_terms = []\n",
    "for docs in doc_corpus.fileids():\n",
    "    words = doc_corpus.words(docs)\n",
    "    word_dict = dict(Counter(words))\n",
    "    for w in range(len(word_dict)):\n",
    "        k = list(word_dict.keys())[w]\n",
    "        v = list(word_dict.values())[w]\n",
    "        if k in all_words:\n",
    "            all_words[k] += v\n",
    "        else:\n",
    "            all_words[k] = v\n",
    "    # 檔名篩掉.txt\n",
    "    doc_names.append(docs[:len(docs) - 4])\n",
    "    doc_terms.append(word_dict)\n",
    "\n",
    "query_words = {}\n",
    "# 將 query 依序 存取內容與檔名\n",
    "query_names = []\n",
    "query_terms = []\n",
    "for queries in query_corpus.fileids():\n",
    "    words = query_corpus.words(queries)\n",
    "    word_dict = dict(Counter(words))\n",
    "    for w in range(len(word_dict)):\n",
    "        k = list(word_dict.keys())[w]\n",
    "        v = list(word_dict.values())[w]\n",
    "        if k in query_words:\n",
    "            query_words[k] += v\n",
    "        else:\n",
    "            query_words[k] = v\n",
    "    # 檔名篩掉.txt\n",
    "    query_names.append(queries[:len(queries) - 4])\n",
    "    query_terms.append(word_dict)\n",
    "\n",
    "filtered_word = {}\n",
    "i = 0\n",
    "for w in all_words.keys():\n",
    "    if w in query_words or all_words[w] > 500:\n",
    "        filtered_word[w] = all_words[w]\n",
    "        word2id[w] = i\n",
    "        id2word[i] = w\n",
    "        i += 1\n",
    "\n",
    "filtered_doc_terms = []\n",
    "for doc in doc_terms:\n",
    "    word_dict = {}\n",
    "    for i in range(len(doc.keys())):\n",
    "        word = list(doc.keys())[i]\n",
    "        if word in filtered_word:\n",
    "            word_dict[word] = doc[word]\n",
    "    filtered_doc_terms.append(word_dict)\n",
    "\n",
    "tf_words = np.zeros((len(filtered_doc_terms),len(filtered_word)))\n",
    "for j in range(len(filtered_doc_terms)):\n",
    "    for i in range(len(filtered_word)):\n",
    "        if id2word[i] in filtered_doc_terms[j]:\n",
    "            tf_words[j][i] = filtered_doc_terms[j][id2word[i]]\n",
    "\n",
    "# 確認長度相符\n",
    "print(len(doc_names),len(doc_terms),len(filtered_doc_terms))\n",
    "print(len(query_names),len(query_terms))\n",
    "print(len(word2id),len(id2word),len(filtered_word))\n",
    "print(len(all_words),len(query_words))\n",
    "print(tf_words.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def initial_p(words_len,documents_len,K = 8):\n",
    "    # pwt[i, k] : p(wi|tk)\n",
    "    pwt = random([words_len, K])\n",
    "    # ptd[k, j] : p(tk|dj)\n",
    "    ptd = random([K, documents_len])\n",
    "    \n",
    "    for i in range(words_len):\n",
    "        normalize = sum(pwt[i, :])\n",
    "        for j in range(K):\n",
    "            pwt[i, j] /= normalize\n",
    "\n",
    "    for i in range(K):\n",
    "        normalize = sum(ptd[i, :])\n",
    "        for j in range(documents_len):\n",
    "            ptd[i, j] /= normalize\n",
    "    \n",
    "    return pwt,ptd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def e_step(pwt,ptd):\n",
    "    words_len = len(pwt)\n",
    "    documents_len = len(ptd[0])\n",
    "    K = len(ptd)\n",
    "    pt_w_d = np.zeros([words_len,documents_len,K])\n",
    "    for j in range(documents_len):\n",
    "        for i in range(words_len):\n",
    "            if id2word[i] in filtered_doc_terms[j]:\n",
    "                sum_pwt_ptd = 0\n",
    "                for k in range(K):\n",
    "                    pt_w_d[i,j,k] = pwt[i,k] * ptd[k,j]\n",
    "                    sum_pwt_ptd += pt_w_d[i,j,k]\n",
    "                if sum_pwt_ptd != 0:\n",
    "                    pt_w_d[i,j] /= sum_pwt_ptd\n",
    "    return pt_w_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def m_step(pwt,ptd,pt_w_d,tf):\n",
    "    words_len = len(pwt)\n",
    "    documents_len = len(ptd[0])\n",
    "    K = len(ptd)\n",
    "    for k in range(K):\n",
    "        sum_c_p = 0\n",
    "        for i in range(words_len):\n",
    "            pwt[i,k] = 0\n",
    "            for j in range(documents_len):\n",
    "                    pwt[i,k] += tf[j][i] * pt_w_d[i,j,k]\n",
    "            sum_c_p += pwt[i,k]\n",
    "        for i in range(words_len):\n",
    "            if sum_c_p == 0:\n",
    "                pwt[i,k] = 1 / words_len\n",
    "            else:\n",
    "                pwt[i,k] /= sum_c_p\n",
    "    for j in range(documents_len):\n",
    "        dj_len = len(tf[j])\n",
    "        for k in range(K):\n",
    "            ptd[k,j] = 0\n",
    "            for i in range(words_len):\n",
    "                    ptd[k,j] += tf[j][i] * pt_w_d[i,j,k]\n",
    "            ptd[k,j] /= dj_len \n",
    "    return pwt,ptd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pwd_pwbg(words_len,documents_len):\n",
    "    pwd = np.zeros((words_len,documents_len))\n",
    "    for j in range(documents_len):\n",
    "        dj_len = len(doc_terms[j])\n",
    "        if dj_len != 0:\n",
    "            for i in range(words_len):\n",
    "                pwd[i,j] = tf_words[j][i] / dj_len\n",
    "    pwbg = np.zeros(words_len)\n",
    "    for i in range(words_len):\n",
    "        pwbg[i] = filtered_word[id2word[i]] / sum(filtered_word.values())\n",
    "    return pwd,pwbg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def em_step(pwt,ptd,pt_w_d,tf,max_iter = 30):\n",
    "    for i in tqdm(range(max_iter)):\n",
    "        pt_w_d = e_step(pwt,ptd)\n",
    "        pwt,ptd = m_step(pwt,ptd,pt_w_d,tf)\n",
    "    return pwt,ptd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pwt_ptd(tf,K = 8,max_iter = 30):\n",
    "    pwt,ptd = initial_p(len(filtered_word),len(doc_terms),K)\n",
    "    pwt,ptd = em_step(pwt,ptd,max_iter,tf)\n",
    "    return pwt,ptd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plsa(pwt,ptd,alpha=0.2,beta=0.4):\n",
    "    ans = \"Query,RetrievedDocuments\"\n",
    "    f = open(\"vsm_result.txt\",\"w+\")\n",
    "    f.write(ans+'\\n')\n",
    "\n",
    "    buf = \"\"\n",
    "    \n",
    "    # Initial\n",
    "    pwd,pwbg = get_pwd_pwbg(len(filtered_word),len(doc_terms))\n",
    "    dot_wd = np.dot(pwt,ptd)\n",
    "    \n",
    "    for i in range(len(query_terms)):\n",
    "        buf = query_names[i] + ','\n",
    "\n",
    "        first = True\n",
    "\n",
    "        pqd = np.ones(len(doc_terms))\n",
    "        qwords = list(query_terms[i].keys())\n",
    "        for word in qwords:\n",
    "            wordid = word2id[word]\n",
    "            for j in range(len(doc_terms)):\n",
    "                awd = alpha * pwd[wordid,j]\n",
    "                bwd = beta * dot_wd[wordid,j]\n",
    "                a_b = (1 - alpha - beta) * pwbg[wordid]\n",
    "                pqd[j] *= (awd+bwd+a_b)\n",
    "                \n",
    "        pqd_sort = pqd.argsort()[::-1]\n",
    "        for s in range(1000):\n",
    "            if first == True:\n",
    "                buf += doc_names[pqd_sort[s]]\n",
    "            else:\n",
    "                buf += (' ' + doc_names[pqd_sort[s]])\n",
    "            first = False\n",
    "        \n",
    "        f.write(buf+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 30/30 [00:14<00:00,  2.10it/s]\n"
     ]
    }
   ],
   "source": [
    "# set K and iter to get pwt and ptd\n",
    "pwt,ptd = get_pwt_ptd(tf_words,K=10,max_iter=30)\n",
    "# plsa\n",
    "plsa(pwt,ptd)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.3 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}